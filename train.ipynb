{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from IPython.display import display,Audio\n",
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    device='cuda'\n",
    "    RATE=16000\n",
    "    percent=0.9\n",
    "    batch_size=4\n",
    "    epoches=20\n",
    "    lr=5e-7\n",
    "    load_pth=None\n",
    "    debug=114514\n",
    "    save_steps=5\n",
    "    length=15360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snaps=[]\n",
    "un_snaps=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Snaps(folder = \"./audios/snaps/\",fuck=False):\n",
    "    if(fuck):\n",
    "        filenames=[folder]\n",
    "    else:\n",
    "        PATH = Path(folder)\n",
    "        filenames=list(PATH.glob('*.wav'))\n",
    "    sum=0\n",
    "    for filename in filenames:\n",
    "        waveform,sample_rate = torchaudio.load(filename)\n",
    "        # print(\"Shape of waveform:{}\".format(waveform.size())) #音频大小\n",
    "        # print(\"sample rate of waveform:{}\".format(sample_rate))#采样率\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, CFG.RATE)\n",
    "        if(sample_rate!=CFG.RATE):\n",
    "            waveform=resampler(waveform)\n",
    "        for channel in waveform:\n",
    "            if(fuck):\n",
    "                return channel\n",
    "            if(len(channel)!=CFG.length):\n",
    "                print(len(channel))\n",
    "                continue\n",
    "            snaps.append(channel)\n",
    "            sum+=1\n",
    "    print(\"Load {0} snaps\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_un_Snaps(filename = \"./audios/un_snaps.wav\"):\n",
    "    cut_steps=CFG.length\n",
    "    waveform,sample_rate = torchaudio.load(filename)\n",
    "    tot_size=waveform.size()\n",
    "    print(\"Shape of waveform:{}\".format(tot_size)) #音频大小\n",
    "    print(\"sample rate of waveform:{}\".format(sample_rate))#采样率\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, CFG.RATE)\n",
    "    if(sample_rate!=CFG.RATE):\n",
    "        waveform=resampler(waveform)\n",
    "    sum=0\n",
    "    for channel in waveform:\n",
    "        data_=channel\n",
    "        for i in range(0,int(floor(data_.shape[0]/cut_steps))):\n",
    "            data=data_[i*cut_steps:(i+1)*cut_steps]\n",
    "            if(len(data)!=CFG.length):\n",
    "                continue\n",
    "            un_snaps.append(data)\n",
    "            sum+=1\n",
    "    print(\"Load {0} un_snaps\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 480 snaps\n",
      "Shape of waveform:torch.Size([2, 11614208])\n",
      "sample rate of waveform:48000\n",
      "Load 504 un_snaps\n",
      "Shape of waveform:torch.Size([2, 29532160])\n",
      "sample rate of waveform:48000\n",
      "Load 1280 un_snaps\n",
      "Shape of waveform:torch.Size([1, 1920000])\n",
      "sample rate of waveform:16000\n",
      "Load 125 un_snaps\n"
     ]
    }
   ],
   "source": [
    "Load_Snaps()\n",
    "Load_un_Snaps(\"audios/un_snaps_1.wav\")\n",
    "Load_un_Snaps(\"audios/un_snaps_2.wav\")\n",
    "# Load_un_Snaps(\"audios/un_snaps_3.wav\")\n",
    "Load_un_Snaps(\"audios/un_snaps_4.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "class SnapSet(Dataset):\n",
    "    def __init__(self,snaps,un_snaps):\n",
    "        self.inputs=[]\n",
    "        self.labels=[]\n",
    "        for (i,snap) in enumerate(snaps):\n",
    "            self.inputs.append(snap)\n",
    "            self.labels.append(1)\n",
    "\n",
    "        for (i,un_snap) in enumerate(un_snaps):\n",
    "            self.inputs.append(un_snap)\n",
    "            self.labels.append(0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # print(item)\n",
    "        return self.inputs[item].to(CFG.device),\\\n",
    "        torch.as_tensor(self.labels[item],device=CFG.device,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(snaps)\n",
    "random.shuffle(un_snaps)\n",
    "tot=int(len(snaps)*CFG.percent)\n",
    "# tot_=int(len(un_snaps)*CFG.percent)\n",
    "left=len(snaps)-tot\n",
    "train_set=SnapSet(snaps[:tot],un_snaps[:2*tot])\n",
    "val_set=SnapSet(snaps[-left:],un_snaps[-left*2:])\n",
    "train_loader=DataLoader(train_set,batch_size=CFG.batch_size,shuffle=True,drop_last=True)\n",
    "val_loader=DataLoader(val_set,batch_size=CFG.batch_size,shuffle=True,drop_last=True)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForXVector were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['tdnn.0.kernel.bias', 'classifier.bias', 'tdnn.1.kernel.weight', 'feature_extractor.bias', 'classifier.weight', 'tdnn.2.kernel.weight', 'projector.bias', 'tdnn.4.kernel.weight', 'tdnn.3.kernel.bias', 'tdnn.0.kernel.weight', 'objective.weight', 'feature_extractor.weight', 'projector.weight', 'tdnn.3.kernel.weight', 'tdnn.1.kernel.bias', 'tdnn.4.kernel.bias', 'tdnn.2.kernel.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
    "pretrained = WavLMForXVector.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "pretrained=pretrained.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Snap(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Snap,self).__init__()\n",
    "        sample_snap=Load_Snaps('audios/snaps/snap0.wav',fuck=True)\n",
    "        self.sample_inputs=feature_extractor(sample_snap, padding=True, return_tensors=\"pt\",sampling_rate=CFG.RATE).to(CFG.device)\n",
    "        # print(self.sample_inputs['input_values'].shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.reshape(CFG.batch_size,-1).cpu().numpy()\n",
    "        tmp=[]\n",
    "        for a in x:\n",
    "            tmp.append(a)\n",
    "        x=feature_extractor(tmp, padding=True, return_tensors=\"pt\",sampling_rate=CFG.RATE).to(CFG.device)\n",
    "        self.sample=pretrained(**self.sample_inputs).embeddings[0]\n",
    "        x=pretrained(**x).embeddings\n",
    "        cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "        similarity = cosine_sim(self.sample, x)\n",
    "        return similarity\n",
    "\n",
    "if CFG.load_pth!=None:\n",
    "    pretrained=WavLMForXVector.from_pretrained(CFG.load_pth)\n",
    "\n",
    "model=Snap()\n",
    "model=model.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean(a):\n",
    "    s=0\n",
    "    t=0\n",
    "    for _ in a:\n",
    "        s+=_\n",
    "        t+=1\n",
    "    return s/t\n",
    "\n",
    "def Accuracy(out,labels):\n",
    "    a=0\n",
    "    b=0\n",
    "    for j in range(len(labels)):\n",
    "        b+=1\n",
    "        if (out[j].item()-0.86)*(labels[j].item()-0.5)>0:\n",
    "            a+=1\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hasee\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]c:\\Users\\Hasee\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\wavlm\\modeling_wavlm.py:1755: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "324it [01:06,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Validation epoch:0 loss=0.020237378776073456 accuracy=0.9791666666666666\n",
      "Saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "324it [01:04,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Validation epoch:1 loss=0.008714120835065842 accuracy=1.0\n",
      "Saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "324it [01:08,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Validation epoch:2 loss=0.055924467742443085 accuracy=0.9513888888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "282it [01:04,  4.37it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer=AdamW(pretrained.parameters(),lr=CFG.lr)\n",
    "\n",
    "accuracy_max=-114514\n",
    "loss_tot=None\n",
    "\n",
    "for epoch in range(CFG.epoches):\n",
    "    model.train()\n",
    "    for i,batch in tqdm(enumerate(train_loader)):\n",
    "        inputs,labels=batch\n",
    "        # print(len(inputs))\n",
    "        # print(labels)\n",
    "        out=model(inputs)\n",
    "        labels=labels.view(-1)\n",
    "        loss=((out-labels)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        if (i+1)%CFG.debug==0:\n",
    "            accuracy=Accuracy(out,labels)\n",
    "            print(\"#Training epoch:{0} idx={1} loss={2} accuracy={3}\".format(epoch, i, loss.item(),accuracy))\n",
    "    \n",
    "    loss_=[]\n",
    "    accuracy_=[]\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    for i,batch in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs,labels=batch\n",
    "            out=model(inputs)\n",
    "            labels=labels.view(-1)\n",
    "            loss=((out-labels)**2).mean()\n",
    "            accuracy=Accuracy(out,labels)\n",
    "            loss_.append(loss)\n",
    "            accuracy_.append(accuracy)\n",
    "\n",
    "    loss_=Mean(loss_)\n",
    "    accuracy_=Mean(accuracy_)\n",
    "    print(\"#Validation epoch:{0} loss={1} accuracy={2}\".format(epoch,loss_,accuracy_))\n",
    "\n",
    "    if(accuracy_>accuracy_max):\n",
    "        accuracy_max=accuracy_\n",
    "        print(\"Saving best model\")\n",
    "        pretrained.save_pretrained(\"./pth/best_pth\")\n",
    "\n",
    "    if((epoch+1)%CFG.save_steps==0):\n",
    "        print(\"Saving {0} model\".format(epoch+1))\n",
    "        pretrained.save_pretrained(\"./pth/{0}_pth\".format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3791a47348c2704d85b6abcab0f16aa85fb80860f783c67628ddd43e9bf228ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
